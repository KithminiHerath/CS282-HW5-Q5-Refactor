{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihVMhKkASqD"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import from dropout_lib modules\n",
    "from dropout_lib.trainer import train_simple, train\n",
    "from dropout_lib.utils import test_cheating\n",
    "from dropout_lib.vis_utils import plot_loss_curve, visualize_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3g-6PC_ASqE"
   },
   "source": [
    "# Problem Intro\n",
    "\n",
    "We will explore the effect of dropout on a simple gradient descent problem. We will train weights $w_1$ and $w_2$ to solve the linear equation $10w_1 + w_2 = 11$, where $w_1$ and $w_2$ are initialized at 0.\n",
    "\n",
    "We formulate this question as an OLS:\n",
    "\n",
    "$$\\min_{\\mathbf{w}} \\lVert \\mathbf{Xw} - \\mathbf{y} \\rVert^2 $$,\n",
    "\n",
    "where $\\mathbf{X}, \\mathbf{y}$ are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNtJT9tcASqF"
   },
   "outputs": [],
   "source": [
    "# Define the data for our simple linear equation: 10*w1 + 1*w2 = 11\n",
    "\n",
    "x = np.array([[10, 1]]) # X represents the coefficients\n",
    "y = np.array([[11]]) # y represents the target output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fM8FLMcASqF"
   },
   "source": [
    "## No Dropout, Least-Square\n",
    "\n",
    "Analytically show what solution we will converge to if we train with gradient descent and an appropriately small learning rate. Take advantage of the fact that when you initialize weights to 0 and train linear regression with gradient descent, you recover the least-squares solution.\n",
    "\n",
    "**Complete the following code** to calculate this solution in python, but you can also use another tool and insert your answer. \n",
    "(HINT: use `np.linalg.pinv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SvQCdo46ASqF",
    "outputId": "b3ad5706-a453-4898-cf5e-05abda322d19"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: YOUR CODE HERE\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "print(f'Least-squares solution: w = {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q59HN5RSBhL0"
   },
   "source": [
    "### Question\n",
    "\n",
    "Please **include the mathematical expression in your written** assignment submission, and **copy and paste the output of the previous cell** into your submission as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8YmZkeZASqG"
   },
   "source": [
    "## No Dropout, Gradient Descent\n",
    "\n",
    "Show training with gradient descent recovers the expected solution. A training loop has been provided for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPA6fgtXCPZV"
   },
   "source": [
    "**Complete the following code to create the linear network for the OLS in PyTorch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "wMQVfwgQASqG",
    "outputId": "b52cefc9-3db4-4130-89dd-db491dc9a742"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: YOUR CODE HERE\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# Initialize all weights to zero for consistent starting point\n",
    "net.load_state_dict({k: v * 0 for k, v in net.state_dict().items()})\n",
    "\n",
    "# Train the network\n",
    "losses = train_simple(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IOAT9SyCxHg"
   },
   "source": [
    "### Question\n",
    "\n",
    "Please **copy and paste the output of the previous cell** (text only) into your submission of the written assignment. **Are the weights obtained by training with gradient descent the same as those calculated using the closed-form least squares method?** Answer this question in your written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk2heQrvASqH"
   },
   "source": [
    "## Dropout, Least-Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BSA2xJcDwTs"
   },
   "source": [
    "Now we add a dropout rate of `p=0.5`, which means that during each forward pass, each input to the network has a 50% probability of being set to `0`. To account for this reduction in the number of inputs, we also need to scale the inputs by `2`. However, during testing, we do not apply any dropout, nor do we scale the inputs.\n",
    "\n",
    "By dropping out each element in the input with a 50% probability, we create a dataset with *four* equally likely inputs, in which $w_1$ is dropped out, $w_2$ is dropped out, both are dropped out, or neither is dropped out. This is our new dataset, represented by `x` and `y`. Using this dataset, we can compute the analytic solution to improve our network's performance.\n",
    "\n",
    "**Complete the following code according to the instructions above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EgGsVh_ASqM",
    "outputId": "6257f7a8-b408-48ba-bd4b-27ffeb983075"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO: YOUR CODE HERE\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "print(\"x =\")\n",
    "print(x)\n",
    "print(\"\\ny =\")\n",
    "print(y)\n",
    "print(f\"\\nLeast-squares solution with dropout: w = {w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_JJhNByEydX"
   },
   "source": [
    "### Question\n",
    "\n",
    "Please **copy and paste the output of the previous cell** (text only) into your submission of the written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLsI5rw7ASqM"
   },
   "source": [
    "## Dropout, Gradient Descent\n",
    "**Add dropout to your network. Implement the Dropout layer below, then run with dropout.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "5BwHdQf-ASqM",
    "outputId": "0809882a-bbea-4d6c-ec60-7e98db7e7f6c"
   },
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    \"\"\"Dropout layer for regularization during training.\n",
    "    \n",
    "    Dropout randomly sets input elements to zero with probability p during\n",
    "    training. During evaluation, the layer passes inputs through unchanged.\n",
    "    This helps prevent overfitting by reducing co-adaptation of neurons.\n",
    "    \n",
    "    Attributes:\n",
    "        p: The dropout probability (fraction of inputs to zero out).\n",
    "            Must be in range [0, 1].\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, p: float = 0.5) -> None:\n",
    "        \"\"\"Initialize the Dropout layer.\n",
    "        \n",
    "        Args:\n",
    "            p: Dropout probability. Default is 0.5 (50% dropout rate).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply dropout to the input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of any shape.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with same shape as input. \n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            ################################################################\n",
    "            # TODO: YOUR CODE HERE\n",
    "            # Implement dropout during training:\n",
    "            ################################################################\n",
    "            ################################################################\n",
    "            pass\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "def init_with_dropout(p: float) -> nn.Sequential:\n",
    "    \"\"\"Initialize a simple linear network with dropout.\n",
    "    \n",
    "    Creates a network with a dropout layer followed by a linear layer\n",
    "    (no bias) mapping 2 inputs to 1 output. Weights are initialized to zero.\n",
    "    \n",
    "    Args:\n",
    "        p: Dropout probability for the dropout layer.\n",
    "        \n",
    "    Returns:\n",
    "        Sequential model with dropout and linear layers, weights set to zero.\n",
    "        \n",
    "    Note:\n",
    "        Zero initialization is used for educational purposes to demonstrate\n",
    "        gradient descent behavior. In practice, proper weight initialization\n",
    "        (e.g., Xavier or He initialization) should be used.\n",
    "    \"\"\"\n",
    "    net = nn.Sequential(\n",
    "        Dropout(p),\n",
    "        nn.Linear(2, 1, bias=False)\n",
    "    )\n",
    "    # Initialize weights with 0\n",
    "    net.load_state_dict({k: v * 0 for k, v in net.state_dict().items()})\n",
    "    return net\n",
    "\n",
    "\n",
    "# Train network with dropout\n",
    "net = init_with_dropout(0.5)\n",
    "losses = train_simple(net)\n",
    "\n",
    "# Plot zoomed-in view of initial training dynamics\n",
    "plot_loss_curve(\n",
    "    losses[:100],\n",
    "    title='Training Loss (First 100 Iterations)',\n",
    "    figsize=(8, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGb3m-UNFjLW"
   },
   "source": [
    "### Question\n",
    "\n",
    "**Describe the shape of the training curve. Are the weights obtained by training with gradient descent the same as those calculated using the closed-form least squares method?** Answer this question in your written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwNuNYtHASqN"
   },
   "source": [
    "## Dropout, Gradient Descent with Larger Batch Sizes\n",
    "\n",
    "Run the cell below, which uses a larger batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "RuBU1l5bASqN",
    "outputId": "532fed19-8477-455c-9bf7-5019439118e1"
   },
   "outputs": [],
   "source": [
    "# Train with larger batch size to see averaging effect of dropout\n",
    "net = init_with_dropout(0.5)\n",
    "losses = train_simple(net, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB6gbJjGK6MZ"
   },
   "source": [
    "### Question\n",
    "\n",
    "**Describe the loss curve and compare it with the loss curve in the last part. Why are they different? Also compare the trained weights with the one calculated by the least-square formula.** Answer this question in your written assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flV_sr1EASqN"
   },
   "source": [
    "# (G) [OPTIONAL]: Sweeping over dropout rate\n",
    "\n",
    "Now, let's see how different dropout rates affect the final solution. Run the cell below to sweep over dropout rates. Since the 4 data points we considered in part (C) are no longer equally likely, we need to weight each data point by its probability of occuring. This turns it into a weighted linear regression problem. The analytic solution for this problem is:\n",
    "\n",
    "$$w = (X^\\top S X)^{-1} X^\\top S y$$\n",
    "\n",
    "where $S$ is the diagonal matrix of probabilities of each data point occuring.\n",
    "\n",
    "Implement the analytic solution in the cell below, and show that the analytic solution matches the empirical solution. You should see that as the dropout rate changes, $w_1$ and $w_2$ change smoothly, except for a discontinuity when dropout rates are 0. Explain this discontinuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "rBEz9lUoASqN",
    "outputId": "43e3ca57-8bbc-4635-b470-b554cbaa3f1c"
   },
   "outputs": [],
   "source": [
    "# Analyze effect of different dropout rates\n",
    "empirical_dropout_rates = [0, 0.1, 0.3, 0.5, 0.7]\n",
    "analytical_dropout_rates = np.arange(0, 0.99, 0.01)\n",
    "losses_empirical: List[float] = []\n",
    "losses_analytical: List[float] = []\n",
    "w1_empirical: List[float] = []\n",
    "w2_empirical: List[float] = []\n",
    "w1_analytical: List[float] = []\n",
    "w2_analytical: List[float] = []\n",
    "\n",
    "for p in analytical_dropout_rates:\n",
    "    # Compute analytical solution\n",
    "    ########################################################################\n",
    "    # TODO: YOUR CODE HERE\n",
    "    ########################################################################\n",
    "    ########################################################################\n",
    "    \n",
    "    # Evaluate analytical solution on original data\n",
    "    x = np.array([[10, 1]])\n",
    "    y = np.array([[11]])\n",
    "    l_analytic = ((x @ w_analytic - y) ** 2).item()\n",
    "    w1_analytical.append(w_analytic[0][0])\n",
    "    w2_analytical.append(w_analytic[1][0])\n",
    "    losses_analytical.append(l_analytic)\n",
    "\n",
    "for p in empirical_dropout_rates:\n",
    "    # Train network empirically\n",
    "    net = init_with_dropout(p)\n",
    "    losses = train_simple(\n",
    "        net, batch_size=1024, itrs=10000, plot=False\n",
    "    )\n",
    "    net.eval()\n",
    "    \n",
    "    # Evaluate on original data\n",
    "    test_loss = ((net(torch.FloatTensor(x)) - torch.FloatTensor(y)) ** 2)\n",
    "    losses_empirical.append(test_loss.item())\n",
    "    w1_empirical.append(net.state_dict()['1.weight'][0][0].item())\n",
    "    w2_empirical.append(net.state_dict()['1.weight'][0][1].item())\n",
    "\n",
    "# Plot analytical vs empirical results\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(analytical_dropout_rates, losses_analytical, label='Analytical')\n",
    "plt.scatter(empirical_dropout_rates, losses_empirical, label='Empirical')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Loss vs Dropout Rate')\n",
    "plt.legend()\n",
    "\n",
    "# Weight comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(analytical_dropout_rates, w1_analytical, label='w1 Analytical')\n",
    "plt.scatter(empirical_dropout_rates, w1_empirical, label='w1 Empirical')\n",
    "plt.plot(analytical_dropout_rates, w2_analytical, label='w2 Analytical')\n",
    "plt.scatter(empirical_dropout_rates, w2_empirical, label='w2 Empirical')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title('Weights vs Dropout Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iY-wEIHkASqN"
   },
   "source": [
    "# (H) [OPTIONAL]: Adding Adam\n",
    "\n",
    "Now, let's add Adam to our network. Run the cell below to train with Adam with and without dropout. Does the solution change? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "JoanwuA8ASqN",
    "outputId": "6a0a41ab-dbf4-4fe9-b128-c767a9e9ee5a"
   },
   "outputs": [],
   "source": [
    "# Compare SGD vs Adam with and without dropout\n",
    "dropout_rates = [0, 0.5]\n",
    "optim_classes = [torch.optim.SGD, torch.optim.Adam]\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "for optim_class in optim_classes:\n",
    "    w1_list: List[float] = []\n",
    "    w2_list: List[float] = []\n",
    "    \n",
    "    for p in dropout_rates:\n",
    "        net = init_with_dropout(p).train()\n",
    "        losses = train_simple(\n",
    "            net, \n",
    "            batch_size=1024, \n",
    "            itrs=10000, \n",
    "            optim_class=optim_class, \n",
    "            plot=False\n",
    "        )\n",
    "        net.eval()\n",
    "        \n",
    "        # Extract learned weights\n",
    "        w1_list.append(net.state_dict()['1.weight'][0][0].item())\n",
    "        w2_list.append(net.state_dict()['1.weight'][0][1].item())\n",
    "    \n",
    "    # Plot weights for this optimizer\n",
    "    axs.plot(dropout_rates, w1_list, label=f'{optim_class.__name__} w1')\n",
    "    axs.plot(dropout_rates, w2_list, label=f'{optim_class.__name__} w2')\n",
    "\n",
    "axs.set_xlabel('Dropout Rate')\n",
    "axs.set_ylabel('Weight Value')\n",
    "axs.set_title('Learned Weights vs Dropout Rate (SGD vs Adam)')\n",
    "axs.legend()\n",
    "axs.set_ylim(0, 4)\n",
    "axs.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N6xjThOASqN"
   },
   "source": [
    "## (I): Dropout on real data\n",
    "\n",
    "There are some unusual features of our previous problem:\n",
    "- We only used a single datapoint\n",
    "- We applied dropout to the inputs to the network, whereas in real problems it's typically applied to hidden units\n",
    "- The network was so small that dropout significantly hurt performance. Typically, networks are large enough that they can fit the data well even with dropout.\n",
    "\n",
    "To see the effect of dropout on a more realistic problem, we'll train a network on the CIFAR10 dataset and add a \"cheating feature.\" In this case, the cheating feature consists of a few pixels in the bottom-right corner of the image which encode the class label*. We want to see how dropout helps the network learn to rely less heavily on this cheating feature. Run the next few cells and comment on how dropout affects the degree to which the network relies on the cheating feature. Which model does better on clean data?\n",
    "\n",
    "*This is obviously a contrived cheating feature, but they can appear in real data -- for instance, if a particular camera was used to capture all images of a certain class, the model might learn to rely on subtle camera artifacts rather than the acutal image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBSQU3PGASqN"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network for CIFAR10 classification.\n",
    "    \n",
    "    A simple CNN architecture with two convolutional layers followed by\n",
    "    max pooling and a fully connected output layer. Dropout can be applied\n",
    "    after each convolutional layer for regularization.\n",
    "    \n",
    "    Architecture:\n",
    "        - Conv2D (3 -> 16 channels, 3x3 kernel) -> MaxPool2D (2x2) -> ReLU\n",
    "          -> Dropout\n",
    "        - Conv2D (16 -> 32 channels, 3x3 kernel) -> MaxPool2D (2x2) -> ReLU\n",
    "          -> Dropout\n",
    "        - Flatten -> Linear (32*8*8 -> 10) --> ReLU -> LogSoftmax\n",
    "    \n",
    "    Attributes:\n",
    "        conv1: First convolutional layer.\n",
    "        conv2: Second convolutional layer.\n",
    "        fc1: Fully connected output layer.\n",
    "        dropout_rate: Probability of dropping activations (0 = no dropout).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate: float = 0.0) -> None:\n",
    "        \"\"\"Initialize the ConvNet.\n",
    "        \n",
    "        Args:\n",
    "            dropout_rate: Dropout probability to apply after conv layers.\n",
    "                Must be in range [0, 1]. Default is 0 (no dropout).\n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        in_channels = 3  # RGB images\n",
    "        \n",
    "        # Convolutional layers with same padding to preserve spatial dimensions\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, padding='same')\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding='same')\n",
    "        \n",
    "        # After two 2x2 max pools, 32x32 image becomes 8x8\n",
    "        img_size = 8\n",
    "        self.fc1 = nn.Linear(32 * img_size * img_size, 10)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 3, 32, 32).\n",
    "            \n",
    "        Returns:\n",
    "            Log probabilities for each of the 10 classes,\n",
    "            shape (batch_size, 10).\n",
    "            \n",
    "        Note:\n",
    "            This network applies more dropout than typical to emphasize\n",
    "            the regularization effect. In practice, dropout is usually\n",
    "            only applied to fully connected layers.\n",
    "        \"\"\"\n",
    "        # First conv block: Conv -> MaxPool -> ReLU -> Dropout\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv1(x), 2)\n",
    "        )\n",
    "        x = torch.nn.functional.dropout(\n",
    "            x, training=self.training, p=self.dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Second conv block: Conv -> MaxPool -> ReLU -> -> Dropout\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2(x), 2)\n",
    "        )\n",
    "        x = torch.nn.functional.dropout(\n",
    "            x, training=self.training, p=self.dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Flatten and classify\n",
    "        img_size = 8\n",
    "        x = x.view(-1, 32 * img_size * img_size)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        \n",
    "        return torch.nn.functional.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "dfbd509fb74545078ecabae45425b779",
      "c76295d5d8af4c98a7264d89c4b718bc",
      "d4e23df771884f93a10a6315dcee1986",
      "5abe9a84dae34df4a74da5b37c73ef2c",
      "da4442a1ecc34ebc898bdba891ca5558",
      "2bf1dcd1dcc545de9dfeeaba76cadae1",
      "240d5286176047e29cf77ffea446b628",
      "3269e46c0d724ed993cdb8f2002dccbf",
      "5dfa60d31f5043d69a6f401ef061f328",
      "2e6e793db1a8426181e00bfec1479789",
      "8da1cfe48a694601bde219a23ceb9d4b"
     ]
    },
    "id": "cHa1Vpg0ASqO",
    "outputId": "382c9ec0-da02-4e92-e778-254095c64ed5"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset with normalization\n",
    "\n",
    "# Normalization constants for CIFAR10 dataset\n",
    "# These are the channel-wise mean and standard deviation computed over\n",
    "# the entire CIFAR10 training set\n",
    "MEAN = [0.4914, 0.4822, 0.4465]\n",
    "STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "# Training data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        'data', \n",
    "        train=True, \n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=64, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Test/validation data loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(\n",
    "        'data', \n",
    "        train=False, \n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "    ),    \n",
    "    batch_size=1000, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6dWjCB6fASqO",
    "outputId": "973c6f10-38f4-46ff-b1a4-02ef57a869db"
   },
   "outputs": [],
   "source": [
    "# Visualize sample images with cheating features\n",
    "visualize_data(train_loader, MEAN, STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "DM247YHMASqO",
    "outputId": "a63dffe5-d5f7-4325-a05c-967c7bedb232"
   },
   "outputs": [],
   "source": [
    "# Train model without dropout\n",
    "model_no_dropout = ConvNet(dropout_rate=0)\n",
    "# Move model to GPU if available\n",
    "model_no_dropout.to(device)\n",
    "\n",
    "print(\"Training model WITHOUT dropout:\")\n",
    "print(\"=\" * 50)\n",
    "train_loss, val_loss = train(\n",
    "    model_no_dropout,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=3e-3\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating reliance on cheating feature:\")\n",
    "test_cheating(model_no_dropout, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "g6qkoAqqASqP",
    "outputId": "dd62db49-1090-4b1d-ba2f-1a03a5cb88c6"
   },
   "outputs": [],
   "source": [
    "# Train model with dropout\n",
    "model_dropout = ConvNet(dropout_rate=0.75)\n",
    "# Move model to GPU if available\n",
    "model_dropout.to(device)\n",
    "\n",
    "print(\"Training model WITH dropout (p=0.75):\")\n",
    "print(\"=\" * 50)\n",
    "train_loss, val_loss = train(\n",
    "    model_dropout,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    lr=3e-3\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating reliance on cheating feature:\")\n",
    "test_cheating(model_dropout, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "077c9ba13c146ebe7a62ed62b34f66019444a78189bfce90d3e43fbe95f9de66"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "240d5286176047e29cf77ffea446b628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bf1dcd1dcc545de9dfeeaba76cadae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e6e793db1a8426181e00bfec1479789": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3269e46c0d724ed993cdb8f2002dccbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5abe9a84dae34df4a74da5b37c73ef2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e6e793db1a8426181e00bfec1479789",
      "placeholder": "​",
      "style": "IPY_MODEL_8da1cfe48a694601bde219a23ceb9d4b",
      "value": " 170498071/170498071 [00:02&lt;00:00, 84092248.47it/s]"
     }
    },
    "5dfa60d31f5043d69a6f401ef061f328": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8da1cfe48a694601bde219a23ceb9d4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c76295d5d8af4c98a7264d89c4b718bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bf1dcd1dcc545de9dfeeaba76cadae1",
      "placeholder": "​",
      "style": "IPY_MODEL_240d5286176047e29cf77ffea446b628",
      "value": "100%"
     }
    },
    "d4e23df771884f93a10a6315dcee1986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3269e46c0d724ed993cdb8f2002dccbf",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5dfa60d31f5043d69a6f401ef061f328",
      "value": 170498071
     }
    },
    "da4442a1ecc34ebc898bdba891ca5558": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dfbd509fb74545078ecabae45425b779": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c76295d5d8af4c98a7264d89c4b718bc",
       "IPY_MODEL_d4e23df771884f93a10a6315dcee1986",
       "IPY_MODEL_5abe9a84dae34df4a74da5b37c73ef2c"
      ],
      "layout": "IPY_MODEL_da4442a1ecc34ebc898bdba891ca5558"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
